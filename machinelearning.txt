隐藏层：大多默认使用Relu函数，可以让梯度下降算法更快，因为sigmoid两个地方平坦
输出层：二元分类sigmoid，可正可负线性函数，y只能取0或正数用Relu
Adam算法是自动选择学习率的梯度下降
数据集分为训练集和测试集，用Jtest和J测试比较，看看泛化能力如何，对于二值回归可以比较准确率

拟合时（房价预估）选取多项式阶数的问题：只分为训练集和测试集，并用测试集根据Jtest来选择阶数，实际会导致阶数更适应测试集，得到的泛化误差实际是过于乐观的。因此\
分为训练集，交叉验证集，测试集，用训练集拟合出几个待选模型（阶数）的w b，用交叉验证集对这些w b进行测试，从而选择阶数d，最后将选好的模型用于测试集，report出最终的泛化误差估计

神经网络的模型选择（层数和单元数的选择）也可以用上述方法分为三个集，其中测试集为泛化能力的公平报告
多类softmax

机器学习：

传统机器学习算法【线性回归（预测房价）、逻辑回归（分类，z应为值等于0的边界）】，核心是通过梯度下降法降低代价函数Jw，需要注意：特征缩放（加快收敛速度），学习率曲线（调节α），防止过拟合三个主要方法（增加样本，特征工程（选择合适的特征，即筛除一些不那么重要的特征），特征正则化（保留多的特征，但当某一个特征的参数过大时惩罚J，惩罚b与否一般没区别）

深度学习（人工神经网络）：如今大数据难以提升传统机器学习算法的性能，而深度学习的性能可以随着数据量的大幅提升而提升     自己学习好的特征（隐含层）

衡量模型怎么样 看看Jtest和Jtrain

softmax分类可以先计算z=wa+b,不把ez算出来，再算误差，可以梯度下降的更快（小数问题）